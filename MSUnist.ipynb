{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 align = \"center\"> MSU-NIST </h1>\n",
        "\n",
        "## CS:3210 Machine Learning Final Project\n",
        "### Professor: Feng Jiang \n",
        "\n",
        "\n",
        "#### Team Members:\n",
        "* **Ling Thang**\n",
        "* **Joaquin Trujillo**\n",
        "\n",
        "#### Project Description:\n",
        "For our Final project we have decided to work on the classic digit recognition problem. Taking a spin on the classic MNSIT dataset we have decided to collect our own dataset from study around the Auraria campus housing the three schools MSU Denver, CU Denver, and CCD. We decided to take this approach because we believe that as machine learning students it is not only important to understand not just the application of machine learning but also the data collection, cleaning, and preprocessing that goes into it. We believe that this will give us a better understanding of the process and the challenges that come with it as well as the importance of data quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naigate to the folder and create classes for Digit Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
          ]
        }
      ],
      "source": [
        "data_dir = 'Data'\n",
        "# classes but only the directories\n",
        "files = os.listdir(data_dir)\n",
        "classes = [f for f in files if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "print(sorted(classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing the data \n",
        "\n",
        "## create two arrays X and y\n",
        "\n",
        "## Loop through the "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9: 100\n",
            "0: 100\n",
            "7: 100\n",
            "6: 100\n",
            "1: 100\n",
            "8: 100\n",
            "4: 100\n",
            "3: 100\n",
            "2: 100\n",
            "5: 100\n",
            "(1000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "# Create empty np arrays to store the images and labels\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Loop through the classes\n",
        "for i, c in enumerate(classes):\n",
        "    files = os.listdir(os.path.join(data_dir, c))\n",
        "    files = [f for f in files if f.endswith('.png')]\n",
        "    \n",
        "    # print the number of files in each class\n",
        "    print(f'{c}: {len(files)}')\n",
        "\n",
        "    # Loop through the images\n",
        "    for f in files:\n",
        "        img = Image.open(os.path.join(data_dir, c, f))\n",
        "        \n",
        "        # Check if image is RGBA, if not, handle it\n",
        "        if img.mode != 'RGBA':\n",
        "            print(f\"Skipping {f}: Not an RGBA image\")\n",
        "            continue\n",
        "        \n",
        "        # Convert RGBA image to RGB\n",
        "        img = img.convert('RGB')\n",
        "        \n",
        "        # Convert RGB image to grayscale\n",
        "        gray_img = img.convert('L')\n",
        "        \n",
        "        # Resize to (28, 28)\n",
        "        gray_img = gray_img.resize((28, 28))\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        gray_img = np.array(gray_img)\n",
        "        \n",
        "        # Expand dimensions to make it (28, 28, 1)\n",
        "        gray_img = np.expand_dims(gray_img, axis=-1)\n",
        "        \n",
        "        # Normalize pixel values to range [0, 1]\n",
        "        gray_img = gray_img / 255.0\n",
        "        \n",
        "        # Append to the list of images and labels\n",
        "        X.append(gray_img)\n",
        "        y.append(i)\n",
        "\n",
        "# Convert lists to arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(X.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 28, 28, 1)\n",
            "(1000,)\n",
            "X_train shape: (800, 28, 28, 1)\n",
            "(800, 10)\n"
          ]
        }
      ],
      "source": [
        "# print the shape of the arrays\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Save the data\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('y_test.npy', y_test)\n",
        "np.save('classes.npy', classes)\n",
        "\n",
        "#print('Data saved')\n",
        "print('X_train shape:', X_train.shape)\n",
        "print (y_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded\n",
            "(800, 28, 28, 1)\n",
            "(200, 28, 28, 1)\n",
            "(800, 10)\n",
            "(200, 10)\n",
            "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "X_train = np.load('X_train.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "classes = np.load('classes.npy')\n",
        "\n",
        "print('Data loaded')\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(sorted(classes))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAHICAYAAAC4fTKEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxyklEQVR4nO3de5jWdZk/8Hs4D3KIgyQIHpCQCDykCJmBGqFlIksQbnmJra2pu6ihHazQ0ixb1HTzKnVTshVXE702tE1XV01MQdQ8pYh5QJGzICeZGZjh98f+tNTv51kH5jPPHF6v6/KP3vd1z3MzPcwz9zzD51Oxffv27QEAAABk0abcAwAAAEBLZvEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGRk8QYAAICMLN5NxGOPPRZHH310dOvWLbp27Rrjxo2LJ554otxjQVncf//9UVFRUfjf/Pnzyz0elEV1dXV861vfin79+kVlZWWMHDky7r777nKPBWWzadOmOP/88+Poo4+Onj17RkVFRfzqV78q91hQFn/+859j8uTJMXDgwOjcuXP07t07Ro8eHbfffnu5R+P/a1fuAYh4/PHH47DDDosBAwbE+eefH3V1dfHzn/88xowZE4888kjsu+++5R4RyuKMM86IESNGvCsbNGhQmaaB8jrppJNizpw5cdZZZ8VHPvKR+NWvfhWf+9zn4r777ovDDjus3ONBo1uzZk1ccMEFsccee8T+++8f999/f7lHgrJZsmRJbNy4MaZOnRr9+vWLt956K2699dYYP358XH311XHKKaeUe8RWr2L79u3byz1Ea3fMMcfEww8/HC+88EL06tUrIiKWL18egwcPjnHjxsWtt95a5gmhcd1///1xxBFHxC233BKTJk0q9zhQdo888kiMHDkyZs6cGeecc05ERFRVVcWwYcOiT58+8dBDD5V5Qmh81dXVsW7duthtt93i0UcfjREjRsSsWbPipJNOKvdo0CTU1tbGQQcdFFVVVbFo0aJyj9Pq+VXzJmDevHkxduzYd5buiIi+ffvGmDFj4o477ohNmzaVcToor40bN8a2bdvKPQaU1Zw5c6Jt27bveseiU6dOcfLJJ8fDDz8cr732Whmng/Lo2LFj7LbbbuUeA5qstm3bxoABA+LNN98s9yiExbtJqK6ujsrKyvflnTt3jpqamnjmmWfKMBWU31e+8pXo1q1bdOrUKY444oh49NFHyz0SlMWf/vSnGDx4cHTr1u1d+SGHHBIR4UwQACIiYvPmzbFmzZp48cUX46c//Wn8/ve/j09/+tPlHovwb7ybhH333Tfmz58ftbW10bZt24iIqKmpiQULFkRExOuvv17O8aDRdejQIb7whS/E5z73uejdu3c8++yzcckll8SnPvWpeOihh+LAAw8s94jQqJYvXx59+/Z9X/52tmzZssYeCYAm6Oyzz46rr746IiLatGkTEydOjCuvvLLMUxFh8W4STj/99DjttNPi5JNPjm9+85tRV1cXP/zhD2P58uUREbFly5YyTwiN69BDD41DDz30nf89fvz4mDRpUuy3335x7rnnxp133lnG6aDxbdmyJTp27Pi+vFOnTu/UAeCss86KSZMmxbJly+I3v/lN1NbWRk1NTbnHIvyqeZNw6qmnxne+85248cYb42Mf+1gMHz48XnzxxfjmN78ZERFdunQp84RQfoMGDYrjjjsu7rvvvqitrS33ONCoKisro7q6+n15VVXVO3UAGDJkSIwdOzZOPPHEd86KOvbYY8N52uVn8W4iLrrooli5cmXMmzcvnnrqqVi4cGHU1dVFRMTgwYPLPB00DQMGDIiamprYvHlzuUeBRtW3b993fgvqb72d9evXr7FHAqAZmDRpUixcuDAWL15c7lFaPb9q3oT06NHjXXex3nPPPdG/f/8YMmRIGaeCpuOll16KTp06+S0QWp0DDjgg7rvvvtiwYcO7Dlh7+yyQAw44oEyTAdCUvf1PkdavX1/mSfCOdxN18803x8KFC+Oss86KNm3830Trsnr16vdlTz75ZMydOzfGjRvn7wStzqRJk6K2tjauueaad7Lq6uqYNWtWjBw5MgYMGFDG6QAot1WrVr0v27p1a/z617+OysrKGDp0aBmm4m95x7sJeOCBB+KCCy6IcePGRa9evWL+/Pkxa9asOProo+PMM88s93jQ6KZMmRKVlZVx6KGHRp8+feLZZ5+Na665Jjp37hwXX3xxuceDRjdy5MiYPHlynHvuubFq1aoYNGhQXH/99fHKK6/EtddeW+7xoGyuvPLKePPNN9852f/222+PpUuXRkTEtGnTonv37uUcDxrN1772tdiwYUOMHj06dt9991ixYkXMnj07Fi1aFJdeeqnfFmwCKrb7l/Zl9+KLL8bpp58ejz/+eGzcuDH23nvvmDp1akyfPj06dOhQ7vGg0f3rv/5rzJ49O/7yl7/Ehg0bYtddd41Pf/rTcf7558egQYPKPR6URVVVVcyYMSNuuOGGWLduXey3335x4YUXxlFHHVXu0aBs9tprr1iyZElh7eWXX4699tqrcQeCMrnpppvi2muvjaeffjreeOON6Nq1axx00EExbdq0GD9+fLnHIyzeAAAAkJV/KAkAAAAZWbwBAAAgI4s3AAAAZGTxBgAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgo3blHgAAWrPq6upkrUOHDoV5RUXFDj1WXV1dYd6mjZ/DA0BOXmkBAAAgI4s3AAAAZGTxBgAAgIws3gAAAJCRxRsAAAAysngDAABARq4TawALFy5M1l544YXCvLa2NtnTr1+/wnzfffdN9vTv3z9Zg+Zg+/btyVpNTU1hvmXLlmTPmjVrCvMVK1Yke1K1zZs3J3tWrlxZmL/22mvJnpSRI0cma1/84hcL89R1U5THvHnzkrV/+Zd/Kcyfe+65ZE9lZWVhPmXKlGTPP//zPydrl112WWFeVVWV7JkxY0Zh3rVr12QPAPBu3vEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFm8AAADIyKnm9XD33XcX5kuXLk32jBgxojCvq6tL9qQ+3m9+85tkT+pk4969eyd7xowZU5j37ds32UPrs2TJkmTtf/7nfwrzUieHr127tjB/6aWXkj3Lly8vzFMnl0dEVFdXF+bbtm1L9qROT+/evXuyp0uXLvV+nG7duhXmHTt2TPZMmDChMHeqeXnMnDmzMP/BD36Q7Jk8eXJhftFFFyV7Vq9eXZjfddddyZ7UzRgREWPHji3MS52Snjp1/eabb072dO7cOVkDgNbIO94AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgo4rt27dvL/cQTcnKlSuTtdR1XtOmTcs1zgeWuoppn332SfakrsM566yzkj3t2rmBrqXatGlTYX755Zcne5566qnCPHVdVkT6+qtSV3alaj179kz2DBgwoDDfbbfdkj2pj9e1a9dkT+rPU+pqsLZt2xbmtbW1yZ727dsna+RxxRVXJGsXXnhhYT5r1qxkz7HHHrvTM72t1FV6pa7mO+SQQwrzZcuWJXuOOuqowvzss89O9px00knJGgANJ3VFcanrYF9//fVkLXUda6nrS/fYY4/CfM8990z2tEbe8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjIEdXvsXDhwmTt0EMPbcRJ6mfx4sWF+cEHH5zsOeiggwrzrVu3Jnucat5y3XPPPYX5m2++mey5+OKLC/O999472VNRUVGvuVqTNm38LLQcnn/++cL8u9/9brLnuuuuK8wb8uTyUnr37r1DtZR+/fola1/84hcL87lz5yZ7nGpODqW+P0mdxNylS5dc40CD27ZtW2H+2GOPJXsWLFhQmJe6DWXgwIHJWupmmlLfo6RmSN3iEhHRv3//ZK2l8l0eAAAAZGTxBgAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMjdUO+xbNmyZO3www9vvEHq6b/+678K86lTpyZ7jjjiiFzj0EStWLEiWbv33nsL82OOOSbZU+o6CmgufvjDHxbmo0ePTvakrthqicaMGVOY33XXXcmejRs3FuZdu3ZtkJlonW6//fZkLXWNXqm/x9DULFq0qDB/9tlnkz0TJ04szBvzuq7UVX+1tbWNNkNz4B1vAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjFrtqeZ1dXX17unUqVOGST64UjOnTjucPn16rnFowlKnSM6ZMyfZ06dPn8L8yCOPbJCZoJzWrFmTrN19992F+fXXX59rnGalX79+hXl1dXWyZ926dYW5U83ZGX/4wx+StS9/+cuNOAnkMXjw4MJ82LBhjTzJ+7344ovJ2vbt2wvzAQMG5BqnWfKONwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMio1V4n1qZN8c8cOnbsmOzZsmVLYd5Y16M89dRTyVr79u0L8z322CPXODRhjz/+eGH+wAMPJHsuuOCCwjz13IqIeOuttwrzN954I9mzfv36wjx1FUVERI8ePQrz1BVoEREdOnRI1mh9Sl2DknqufPzjH881TrPSuXPnwrx79+7JntSVhvBBpK5PXb58ebKnd+/eucaBRtMUvndJ7Rup798iIkaOHFmYp/at1spnAwAAADKyeAMAAEBGFm8AAADIyOINAAAAGVm8AQAAIKNWe6p5ypAhQ5K1P/7xj4X50Ucf3aAzpE5P//Wvf53sGT16dGHuNMHWKXXa+LZt25I9d999d2H+hz/8IdmzZMmSwnzr1q3JntTp5VVVVcmetWvXFuY9e/ZM9kyePLkwHzNmTLKHlqvUacipmyka68aKpi71daPUyeUVFRW5xqEVqKmpKczXrFmT7KmsrMw1DjSa1HO/1M1GS5cuLczbtUuveaVurNlzzz0L809+8pPJHvvGB+OzBAAAABlZvAEAACAjizcAAABkZPEGAACAjCzeAAAAkJHFGwAAADJyndh7jBgxIlm7/fbbC/Pf/va3yZ7BgwcX5ps2bUr2vPrqq4V5jx49kj2f+tSnkjVan1GjRhXmpa75evHFFwvzXXfdNdmz//77F+Z9+/ZN9uyyyy6Feanrh1LXlt1zzz3Jnm9/+9uF+RlnnJHs+fu///tkjeaturo6WUtdi+VKrP9V6utGSqlrbOD/krpSqdQVdh06dMg1DjSab3zjG4X5LbfckuxJXf/Vu3fvZE+p79NOOeWUwtyVYTvPZxAAAAAysngDAABARhZvAAAAyMjiDQAAABlZvAEAACAjx46+R6kT+4477rjC/KWXXkr2LF26tDDv1q1bsufYY48tzBcvXpzsGTJkSLJG69OxY8fCfOzYscmeUrVy69WrV2H+8Y9/PNnTs2fPwvyyyy5L9gwdOrQwT53eTvNRWVmZrKVOPC91gnJrsnHjxsK81Oe0e/fuucahFUh9L7Zhw4ZkT+okdGhOvvKVrxTmkyZNSvakvkcptWusXr06WZs9e3ZhPmDAgGTPhz/84WSNv/KONwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjIdWINYODAgTtUS0ld3VLqqozevXvX+3GgJZs6dWphfttttyV75syZU5i7Tqz525GvkVu2bEnWOnfuvDPjNCuvvvpqYV7qqppddtkl1zi0AnV1dYX5pk2bkj1t27bNNQ40mgMOOKBRHmfXXXdN1lJfv1977bVkj+vEPhjveAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGTkVPMmaNGiRYX5Hnvs0ciTQPPVvn37wnzEiBHJnieeeKIwr62tTfY4Sbd52GuvvZK1qqqqwvypp55K9hxxxBE7O1Kz8fjjjxfmH/3oR5M9bdr4uT47LvX8KfX1ttQtBPC3Uqfmt6avW2+++Way9sc//rEwnzBhQp5hWpHW8wwDAACAMrB4AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZOQ6sSboySefLMz333//Rp4EWp7UNSIR6etoWtMVIy1Vv379krXU1WA/+clP6t3TXL311lvJ2jPPPFOYn3baabnGoZXr3LlzYV5ZWZns2bBhQ65xaMJSr+k1NTXJnnbtitef1vRaf/755ydrQ4cOLcx33XXXXOO0Gq3nGQYAAABlYPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGTjUvk1InKy9fvrwwnzJlSq5xoNVYsWJFsta/f//CvKKiItc4NAHTp08vzMeNG5fsWbBgQWE+cuTIBpmpsd17773J2qZNmwrzQw45JNc4tHKp06VTN09ERKxbty7XODRhqedKhw4d6t3TFLz88suF+Yc//OFkT+oWgBkzZiR7nnvuuWSt1I0e7Jym+8wDAACAFsDiDQAAABlZvAEAACAjizcAAABkZPEGAACAjCzeAAAAkJHrxMpk5cqVyVplZWVh3rVr11zjQIuzdu3awvz+++9P9pxwwgmZpqEpO/jggwvzyZMnJ3uOP/74wnzOnDnJnoMOOqh+g2Xw2muvFeYzZ85M9px++umFudckGluXLl2StVJXjdH6NOUrw2666aZk7bHHHivMJ06cmOz5wQ9+UJj/5S9/Sfb88Y9/TNY6deqUrLFzmu6zEgAAAFoAizcAAABkZPEGAACAjCzeAAAAkJHFGwAAADJyqnmZPPvss8nakCFDGnESaJlmz55dmK9bty7ZM2XKlFzj0AxdeumlydoZZ5xRmKdOSI+IOOmkkwrz6dOnJ3v22WefwrympibZc9999yVrl112WWF+4IEHJntKnaYLjWn9+vXJ2ubNmxtxEthxpZ6rl1xySWF+zTXXJHuOO+64wvw3v/lNsqdbt27JGvl4xxsAAAAysngDAABARhZvAAAAyMjiDQAAABlZvAEAACAjizcAAABkVLF9+/bt5R6iNfrlL3+ZrB155JGF+cCBA3ONA83S0qVLk7X999+/MJ88eXKy56qrrtrpmWjdbr/99mTt29/+dmG+aNGiZE/fvn0L823btiV76urqkrVzzjmnMP/617+e7Gnfvn2yBo2poqIiWbv44osL829961u5xoEGt2HDhsK8U6dOyZ4OHTrkGocG5h1vAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjNqVe4DWasuWLcla6hRbKIdSFx+8/PLLhXmvXr2SPd27d2+wGf7pn/4p2VNdXV2Yn3vuufV+fPigjj322GTtmGOOKcyfeuqpZM8rr7xSmLdt2zbZM2LEiGRtt912S9agqfvP//zPZG3IkCGNNwhk0q1bt3KPQEbe8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjI4g0AAAAZuU6sTD70oQ8la+3bt2+8QWAnzJ8/vzBPXYEUEXHiiSfW+3Euvvjiwnzu3LnJnttuu60w33PPPev9+NAQ2rQp/ln3AQcckOwpVYPW5rjjjiv3CAA7zDveAAAAkJHFGwAAADKyeAMAAEBGFm8AAADIyOINAAAAGTnVvEz69euXrNXV1TXiJFBaRUVFsjZ69OjCfOHChcmeE044oTBfsmRJsmf16tWF+eWXX57s+bu/+7tkDQAAGpN3vAEAACAjizcAAABkZPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFdu3b99e7iFao02bNiVrnTt3LszbtPFzEpqHDRs2JGsPPfRQYb5q1apkz8EHH1yYDx06tH6DAQBAGdjkAAAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMnKqOQAAAGTkHW8AAADIyOINAAAAGVm8AQAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZGTxBgAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZGTxBgAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFu8m4M9//nNMnjw5Bg4cGJ07d47evXvH6NGj4/bbby/3aFA2jz32WBx99NHRrVu36Nq1a4wbNy6eeOKJco8FZeF1At6vuro6vvWtb0W/fv2isrIyRo4cGXfffXe5x4Im46KLLoqKiooYNmxYuUchLN5NwpIlS2Ljxo0xderUuOKKK2LGjBkRETF+/Pi45ppryjwdNL7HH388DjvssHjppZfi/PPPj/POOy9eeOGFGDNmTDz//PPlHg8andcJeL+TTjopLrvssvjyl78cV1xxRbRt2zY+97nPxYMPPlju0aDsli5dGj/60Y9il112Kfco/H8V27dv317uIXi/2traOOigg6KqqioWLVpU7nGgUR1zzDHx8MMPxwsvvBC9evWKiIjly5fH4MGDY9y4cXHrrbeWeUIoP68TtGaPPPJIjBw5MmbOnBnnnHNORERUVVXFsGHDok+fPvHQQw+VeUIor+OPPz5Wr14dtbW1sWbNmnjmmWfKPVKr5x3vJqpt27YxYMCAePPNN8s9CjS6efPmxdixY99ZuiMi+vbtG2PGjIk77rgjNm3aVMbpoGnwOkFrNmfOnGjbtm2ccsop72SdOnWKk08+OR5++OF47bXXyjgdlNcDDzwQc+bMicsvv7zco/A32pV7AP5q8+bNsWXLlli/fn3MnTs3fv/738eUKVPKPRY0uurq6qisrHxf3rlz56ipqYlnnnkmRo0aVYbJoLy8TsD/+tOf/hSDBw+Obt26vSs/5JBDIiLiiSeeiAEDBpRjNCir2tramDZtWnz1q1+N4cOHl3sc/obFuwk5++yz4+qrr46IiDZt2sTEiRPjyiuvLPNU0Pj23XffmD9/ftTW1kbbtm0jIqKmpiYWLFgQERGvv/56OceDsvE6Af9r+fLl0bdv3/flb2fLli1r7JGgSbjqqqtiyZIlcc8995R7FN7Dr5o3IWeddVbcfffdcf3118dnP/vZqK2tjZqamnKPBY3u9NNPj8WLF8fJJ58czz77bDzzzDNx4oknxvLlyyMiYsuWLWWeEMrD6wT8ry1btkTHjh3fl3fq1OmdOrQ2b7zxRpx33nkxY8aM2HXXXcs9Du9h8W5ChgwZEmPHjo0TTzzxnX/Heuyxx4bz72htTj311PjOd74TN954Y3zsYx+L4cOHx4svvhjf/OY3IyKiS5cuZZ4QysPrBPyvysrKqK6ufl9eVVX1Th1am+9973vRs2fPmDZtWrlHoYDFuwmbNGlSLFy4MBYvXlzuUaDRXXTRRbFy5cqYN29ePPXUU7Fw4cKoq6uLiIjBgweXeTpoGrxO0Fr17dv3nd+C+ltvZ/369WvskaCsXnjhhbjmmmvijDPOiGXLlsUrr7wSr7zySlRVVcXWrVvjlVdeibVr15Z7zFbN4t2Evf1rUuvXry/zJFAePXr0iMMOO+ydw0Huueee6N+/fwwZMqTMk0HT4HWC1uqAAw6IxYsXx4YNG96Vv30WyAEHHFCGqaB8Xn/99airq4szzjgj9t5773f+W7BgQSxevDj23nvvuOCCC8o9ZqvmHu8mYNWqVdGnT593ZVu3bo1Ro0bFc889F6tWrfKrtbR6N998cxx//PFxySWXxNlnn13ucaBReZ2Ad1uwYEGMGjXqXfd4V1dXx7Bhw6JXr14xf/78Mk8IjWvNmjXx4IMPvi//3ve+Fxs3bowrrrgi9tlnHyedl5FTzZuAr33ta7Fhw4YYPXp07L777rFixYqYPXt2LFq0KC699FLfTNHqPPDAA3HBBRfEuHHj3vkGatasWXH00UfHmWeeWe7xoNF5nYB3GzlyZEyePDnOPffcWLVqVQwaNCiuv/76eOWVV+Laa68t93jQ6Hr37h0TJkx4X/72Xd5FNRqXd7ybgJtuuimuvfbaePrpp+ONN96Irl27xkEHHRTTpk2L8ePHl3s8aHQvvvhinH766fH444/Hxo0bY++9946pU6fG9OnTo0OHDuUeDxqd1wl4v6qqqpgxY0bccMMNsW7duthvv/3iwgsvjKOOOqrco0GTcfjhh8eaNWvimWeeKfcorZ7FGwAAADJyuBoAAABkZPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEbtyj0A0PK8+uqrydpjjz1WmC9fvjzZ07Fjx8L8E5/4RLJn6NChyRoALcvChQsL8wULFiR7KisrC/MJEyYke3r16lWvuQDe5h1vAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjJxqDgBAk/H8888X5scff3yyp2/fvoX5EUcckexZvXp1YX7UUUcle77//e8X5p///OeTPQARFm/g//DII48ka7NmzSrMKyoqkj2jRo0qzA8++OBkT01NTWF+yy23JHtS3zilHh8AAHLxq+YAAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjI4g0AAAAZVWzfvn17uYdoLpYtW1aYv/rqq/X+WF26dEnWevbsWa88IqJTp071noHWp6qqKllLXZGyZs2aZM+ZZ55ZmA8fPrxec+2o9evXJ2s333xzYX7KKafkGgeAD2jJkiXJWupWiuuuuy7Zc+ihh+70TG974403krXJkycX5vfee2+DPT40N3Pnzi3MBw0alOwZOnRornGaLO94AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZNSu3AMAQGu2atWqZG3p0qWF+bp165I9bdqkf6b+4Q9/uDAfMmTIDn08+L/U1dUV5ieccEKy50tf+lJhvvvuuzfITP+XXr16JWtdu3YtzEv9nezRo8dOzwTltm3btmTt1ltvLcwPP/zwZE9rPNW81S7eqSfPbbfdlux5/vnnC/NSX1C3bNlSr48VEbF69erCfO3atcmeAw44oDCfOXNmsscVZC1X6oq7s846K9kzYcKEwvzEE09sgInyKHU92oEHHliYl7pBsaKiYqdnAgCA9/JjbAAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZGTxBgAAgIxa9KnmpY69nz17dmHep0+fZM+MGTN2eqadUeoE54kTJxbmy5YtS/YMHDhwp2eifBYuXJispZ6rF154YbJnxIgROz1TY0tdjVSqdvHFFyd7HnjggcL8d7/7XbLHSegt19atW5O11PPoT3/6U7KnpqamMO/SpUuyp3fv3oV5z549kz3t27dP1lLXk6VuQoiIuPTSSwvz1ngVDPV3zTXXFOb77rtvsuf888+vd8+ee+5Zv8F2UGVlZWGeusUmwnVivFupW4pKfW0vtzfeeCNZS+1PpV6PWiPveAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGTUok81B4AdVVdXl6w99NBDhfm5556b7PnEJz5RmDeFU1+ffvrpZO2SSy4pzK+77rpc49DMbNq0KVmbP39+YX7MMccke2655ZbCfMqUKfUbDMoodctFqVuSpk+fXpgfeeSRDTLTzlixYkWylrqdo02b9Hu81dXVhXnHjh3rN1gz0qIX7/vuuy9Z22OPPQrzI444Itc4O23OnDnJWuobutSfk+bjrrvuKsyvuOKKZE/q+hbPh9LXYfz+978vzDdv3pzsKXUVFAAARPhVcwAAAMjK4g0AAAAZWbwBAAAgI4s3AAAAZGTxBgAAgIxaxKnmzz//fGH+6KOPJnsmTpyYa5ydNnfu3ML85ZdfTvZ84xvfKMzbtWsR/xe3eHfccUey9vOf/7ww//d///dkT69evXZ6ppaqf//+yVr37t0L81122SXXODRhpa40SZ2A31wNHz48WSt1rRpERPz3f/93sjZhwoTCfOvWrcmeYcOG7exI2dTU1BTmLfkKJHbMww8/XJhfeOGFyZ6m/H376tWrk7XU8//NN9/MNE3z5B1vAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjJru0XkAQKPYtGlTstamjZ/RU9ozzzyTrE2fPr0wv+GGG5I9++23307PlEttbW1h3r59+0aehKZu7dq1hfmee+6Z7OnZs2eucXZaqdeJdevWFealTmlvjTcBNJvFe9u2bcnaj3/848K8srIy2bNx48adnmlnXHvttcnaqlWrCvNzzz032dOUrx/grx555JHC/LLLLkv23HbbbYX5hz70oYYYqcVKXfd0++23J3t69OiRaxwAAFoxP8YGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGFm8AAADIqEUchX3qqacW5gcffHCyp7FOAf/Od75TmJc6cT11ermTy5uHUtctzJgxozAvda1Kazm9PHWaf0TEgw8+WJjPnTs32dO3b9/C/Oabb072jBo1qjB/+umnkz1N+dob+KDuv//+ZG3w4MGNNwhNWl1dXWGeumIrIqJLly6FeermiYj0NUxNQerP2hqvRqK0mpqawrxDhw6NPEnDqK6uTtbuuOOOwvxnP/tZrnGaJe94AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZOSYbABoJdavX1+YX3XVVcmef/u3f8s1Di3E5MmT692Tei5GRHTt2nVnxtlp27ZtS9ZSJ7s71Zz32rp1a2Hetm3bRp6kYSxbtixZW7x4cWF++OGHZ5qmeWo2i3epq7RSVwA1lmnTpiVrqS/eP/rRj3KNQ5lt3LgxWUtdmTVv3rxkz0c+8pHCvFu3bsmeioqKwnzLli3JnjfffLMwL3XN16uvvlqYv/LKK8melStXFualXoiGDx9emH/3u99N9qQ+b6V86lOfKsxvvPHGZI/rxAAA+L/4VXMAAADIyOINAAAAGVm8AQAAICOLNwAAAGRk8QYAAICMms2p5k3B5ZdfXpg/+uijyZ5Sp1XTMvXt2zdZ+/Wvf12Yz549O9lzxx13FOabN2+u32AR0aFDh2Ste/fuhXm/fv2SPXvttVdhPnLkyGTP4MGDC/NevXolexrLl770pcL8tNNOS/ZcfPHFucaBHbJ8+fJk7Wtf+1phfvbZZyd7Sn1No3Vp06b4/Zphw4Yle1LXb7Vv3z7ZU6rWGEpddZb6HMB7pW42aurPodTf2enTpyd7fvKTnxTmTf3P2th8NgAAACAjizcAAABkZPEGAACAjCzeAAAAkJHFGwAAADJyqjkANDPPPvtsslbqhPKLLrqoMP/4xz++0zNBkTfeeKMw79GjR7Jn69athXnqtOWIhj09udTNAKXmhr9VW1tb7hF2yI9+9KPCfJ999kn29O/fP9c4LYrF+z2qqqqStZkzZxbmDz/8cLKnXTufYv5q+PDhhbkrqZqGT37yk4X5qlWrkj2pb9BcwQQAwNv8qjkAAABkZPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGjtyuh65duxbmr776arKnT58+9X6cjRs3FuZr165N9qSu3thzzz2TPak/D7RWlZWVhfl+++2X7LnjjjsK83/8x39skJlo3R588MHC/Mtf/nKy57rrrkvWUle+rFmzJtnTs2fPwrwhr2+i5Xr99dcL89122y3Zs3Tp0sL8rbfeSvZ06dKlfoOVsHjx4mRt4MCBDfY4UC7z5s1L1lJ/Z88888xkz+bNm3d6ptbAqyYAAABkZPEGAACAjCzeAAAAkJHFGwAAADKyeAMAAEBGTjUHgCaqb9++hfnEiROTPXPnzk3Wbr311sI8dZtGRER1dXVhPmjQoGTP+eefX5h37Ngx2UPLtHz58sI89dyOiFixYkVhvmnTpmRPQ55q/uSTTyZrw4YNa7DHoXWqq6trlMcpdRtS6mt0RMRtt91WmN90003JnvXr13/wwVoxi/d7dOrUKVn7xS9+UZj/7Gc/S/akrmEp9TgpVVVVydrKlSsL89deey3Z85nPfKYwP/fcc5M9riCjNZoyZUqy9h//8R+FuevEAAB4m181BwAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMip5vVwxBFH1CuPiNi2bVu98ogdO/E8Zd26dcnaBRdcUJgfeeSRyZ577723MHfaOS3Z+PHjk7Xvfve7hXnqCqYIVyrxwe2zzz6F+U9/+tNGmyF19c0vf/nLZM/VV19dmJ9xxhkNMhPNR+pqsD59+iR7Ul8jS10n1pCefvrpZG3q1KmNMgPNX9u2bQvzrVu3JnsqKyvr/Tipr9Enn3xysud73/tesvahD32oMN9tt92SPffff3+yxl95xxsAAAAysngDAABARhZvAAAAyMjiDQAAABlZvAEAACAjizcAAABk5DqxzNq1K/4Up/KG1qNHj2QtdR3Naaedluw577zz6vWxoCXYY489krXu3bsX5gsXLkz2HHbYYTs9EzSWNm2Kf0a///77J3vuvPPOXOPQzKxZs6YwT12VF5G+TqyqqqpBZnrbli1bCvONGzcme/baa68GnYGWK/W1s9SVwjvi1FNPLcxLXQ9cqpZS6u/srFmz6v3xWiPveAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGTkVHPe57Of/Wyydu211zbiJND0jR07tjD/3e9+l+xxqjnNyVtvvVWYX3jhhcmeK6+8Mtc4NDObNm0qzLt06ZLsadu2bWFeV1fXIDO9bd68eYX5sGHDkj2pk6rhvTp37lyY78jp/KVuHOrTp09hPm3atHo/TimDBg1K1lasWNGgj9VS+eoBAAAAGVm8AQAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMnKdWCuWus7gvPPOS/Z8/etfzzUONEsTJkwozEtdtfTjH/840zSwY6qrq5O1z3/+84X5GWeckezZa6+9dnYkWoitW7c22MdKXc+0o+64447C/Atf+EKDPg6t09ChQwvzUlfz/uUvfynMhw8fnuw555xz6jfYDqqsrEzWdt1118J80aJFyZ4hQ4bs9EzNjXe8AQAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMnKqeQv3/PPPJ2snnHBCYV7qNM+pU6fu9EzQknziE58ozF9//fVkT+oE6Y4dOzbITJBSV1dXmI8dOzbZk/q6nzrRH/5W165dC/NVq1Yle9q0KX5faPfdd6/342/bti1Ze+655wrzUaNG1ftx4L0mT55cmN94443Jns9+9rOF+cEHH9wgM+WS2h1uuOGGZM8Pf/jDXOM0Wd7xBgAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMjiDQAAABm16OvENm7cmKylrrdorq666qrC/Lzzzqt3z8SJExtkJmgNdt1118K8R48eyZ5FixYV5vvvv3+DzETrlroyLCLi0EMPLcxLXQ321a9+dWdHohUbMGBAYf7CCy8ke1LXDFVWVtb78e+8885k7aMf/Whh7mpHcvrSl75U7hEaXOrqtNT1aBHpq1Xbt2+f7EldNdhcNO/pAQAAoImzeAMAAEBGFm8AAADIyOINAAAAGVm8AQAAIKMWfar5Sy+9lKx97GMfK8zbtSv/p6SqqqownzJlSrJnyZIlhfn8+fOTPQMHDqzfYMAHdvDBBydrDz/8cGHuVHM+qOXLlydro0aNStb+4R/+oTD/9re/vdMzQZGDDjqoML/pppuSPb169Wqwx7/iiiuStV/84hcN9jjQmnXp0qUwL3VT0syZMwvzYcOGJXvGjx9fmDeX086bx5QAAADQTFm8AQAAICOLNwAAAGRk8QYAAICMLN4AAACQkcUbAAAAMir/3VkZPf3008navvvuW5g39HViqavBZs+enez5+te/XpiffPLJyZ7f/va39RsMyGr06NHJ2mOPPdaIk9CY6urqkrUFCxYU5sOHD0/2/PSnPy3Mv//97yd7fvnLXyZrX/nKV5I1yCH1/da6deuSPc8//3y9PlZExGWXXVaYH3jggcmeQYMGJWvAzjv99NOTtYsuuqgwT135HNF8rg1Lad7TAwAAQBNn8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYt+lTz2traZG3KlCmFeb9+/ZI9a9euLcyffPLJZM+OnMx56623Fuaf+cxnkj1A07Lffvsla3feeWcjTkJj2rhxY7J26KGHFuYdO3ZM9owdO7YwX7JkSbKnf//+yRo0ttQpxJdffnmy54tf/GJhXup7p61btxbmt9xyS3o4IKtSr28XXHBBI07SNHjHGwAAADKyeAMAAEBGFm8AAADIyOINAAAAGVm8AQAAICOLNwAAAGTUoq8Tmzp1arLWp0+fwnzevHnJntRVY1/4wheSPaNGjSrM99hjj2QP0Pz17NkzWevcuXMjTkJj6t69e7K2cuXKeveUuooFmrMRI0Yka3fddVdhnrqiNSJizJgxOz0TQE7e8QYAAICMLN4AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMioYvv27dvLPQRAS3PTTTcla+3aFV8oMWnSpFzjAABQRt7xBgAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMjiDQAAABm5Tgwgg02bNiVrXbp0acRJAAAoN+94AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZORUcwAAAMjIO94AAACQkcUbAAAAMrJ4AwAAQEYWbwAAAMjI4g0AAAAZWbwBAAAgI4s3AAAAZGTxBgAAgIws3gAAAJCRxRsAAAAysngDAABARhZvAAAAyMjiDQAAABlZvAEAACAjizcAAABkZPEGAACAjP4ffrN9bvRF8/8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# print a random sample images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(10):\n",
        "    idx = np.random.randint(0, X_train.shape[0])\n",
        "    axes[i].imshow(X_train[idx], cmap = \"gray\")\n",
        "    axes[i].set_title(classes[np.argmax(y_train[idx])])\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class 0: 80 samples\n",
            "Class 1: 80 samples\n",
            "Class 2: 80 samples\n",
            "Class 3: 80 samples\n",
            "Class 4: 80 samples\n",
            "Class 5: 80 samples\n",
            "Class 6: 80 samples\n",
            "Class 7: 80 samples\n",
            "Class 8: 80 samples\n",
            "Class 9: 80 samples\n",
            "Number of samples in balanced dataset: 800\n",
            "Shape of augmented dataset: (8000, 28, 28, 1)\n",
            "Shape of augmented labels: (8000, 10)\n",
            "Number of classes: 10\n",
            "Number of unique classes in y_train: 10\n",
            "Shape of X_train: (800, 28, 28, 1)\n",
            "Shape of y_train: (800, 10)\n"
          ]
        }
      ],
      "source": [
        "# data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "      #rotation_range=10,  \n",
        "        zoom_range = 0.10,  \n",
        "        width_shift_range=0.1, \n",
        "        height_shift_range=0.1)\n",
        "\n",
        "augmented_X = []\n",
        "augmented_y = []\n",
        "\n",
        "def balance_dataset(X, y):\n",
        "    balanced_X = []\n",
        "    balanced_y = []\n",
        "    num_classes = y.shape[1]\n",
        "    for c in range(num_classes):\n",
        "        indices = [i for i, label in enumerate(y) if np.argmax(label) == c]\n",
        "        print(f\"Class {c}: {len(indices)} samples\")\n",
        "        if len(indices) < 1:\n",
        "            continue  # Skip if the class has no samples\n",
        "        num_samples_per_class = min(np.sum([1 for label in y if np.argmax(label) == c]), len(indices))\n",
        "        selected_indices = np.random.choice(indices, num_samples_per_class, replace=True)\n",
        "        balanced_X.extend([X[i] for i in selected_indices])\n",
        "        balanced_y.extend([y[i] for i in selected_indices])\n",
        "    return balanced_X, np.array(balanced_y)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_X_train, balanced_y_train = balance_dataset(X_train, y_train)\n",
        "print(\"Number of samples in balanced dataset:\", len(balanced_X_train))\n",
        "\n",
        "# Now, apply data augmentation to the balanced dataset\n",
        "# Generate augmented data for each sample in the balanced dataset\n",
        "for i in range(len(balanced_X_train)):\n",
        "    X_train_example = balanced_X_train[i].reshape((1, 28, 28, 1))\n",
        "    y_train_example = balanced_y_train[i].reshape((1, 10))  # Reshape to (1, 10) as it's one-hot encoded\n",
        "    num_augmented_samples = 10  # You can adjust the number of augmented samples per original sample\n",
        "    for _ in range(num_augmented_samples):\n",
        "        X_train_augmented, y_train_augmented = datagen.flow(X_train_example, y_train_example).__next__()\n",
        "        # Reshape augmented data to remove extra dimension\n",
        "        X_train_augmented = X_train_augmented.squeeze(axis=0)\n",
        "        y_train_augmented = y_train_augmented.squeeze(axis=0)\n",
        "        augmented_X.append(X_train_augmented)\n",
        "        augmented_y.append(y_train_augmented)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "augmented_X = np.array(augmented_X)\n",
        "augmented_y = np.array(augmented_y)\n",
        "\n",
        "# Print the shape of the augmented dataset\n",
        "print(\"Shape of augmented dataset:\", augmented_X.shape)\n",
        "print(\"Shape of augmented labels:\", augmented_y.shape)\n",
        "print(\"Number of classes:\", len(classes))\n",
        "print(\"Number of unique classes in y_train:\", len(np.unique(np.argmax(y_train, axis=1))))\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9: 800\n",
            "0: 800\n",
            "7: 800\n",
            "6: 800\n",
            "1: 800\n",
            "8: 800\n",
            "4: 800\n",
            "3: 800\n",
            "2: 800\n",
            "5: 800\n"
          ]
        }
      ],
      "source": [
        "for i, c in enumerate(classes):\n",
        "    print(f'{c}: {np.sum(np.argmax(augmented_y, axis=1) == i)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "def intialize_model_train(model,epoch, batch_size):\n",
        "\tprint(\"******* training network *******\")\n",
        "# Compile the model with Adam optimizer\n",
        "\tadam_optimizer = Adam(learning_rate=0.001)  # You can adjust the learning rate as needed\n",
        "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimizer, metrics=[\"accuracy\"])\n",
        "\t# go through the data 5 times with 128 batch sizes\n",
        "\tH = model.fit(augmented_X,augmented_y, validation_data=(X_test, y_test),\n",
        "\t\tepochs=epoch, batch_size=batch_size)\n",
        "\n",
        "\t# evaluate the network\n",
        "\tprint(\"******* evaluating network *******\")\n",
        "\tpredictions = model.predict(X_test, batch_size=batch_size)\n",
        "\tprint(classification_report(y_test.argmax(axis=1),\n",
        "\t\tpredictions.argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******* training network *******\n",
            "Epoch 1/9\n",
            "80/80 [==============================] - 13s 148ms/step - loss: 2.3047 - accuracy: 0.0921 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 2/9\n",
            "80/80 [==============================] - 10s 126ms/step - loss: 2.3027 - accuracy: 0.1034 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 3/9\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 2.3030 - accuracy: 0.0965 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 4/9\n",
            "80/80 [==============================] - 13s 166ms/step - loss: 2.3030 - accuracy: 0.0881 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 5/9\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 2.3030 - accuracy: 0.0897 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 6/9\n",
            "80/80 [==============================] - 11s 143ms/step - loss: 2.3028 - accuracy: 0.1005 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 7/9\n",
            "26/80 [========>.....................] - ETA: 8s - loss: 2.3023 - accuracy: 0.1112"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mintialize_model_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[25], line 22\u001b[0m, in \u001b[0;36mintialize_model_train\u001b[0;34m(model, epoch, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39madam_optimizer, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# go through the data 5 times with 128 batch sizes\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43maugmented_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# evaluate the network\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m******* evaluating network *******\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "intialize_model_train(model,9,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "def Save_Model(model):\n",
        "    #os.chdir(\"SavedModels\")\n",
        "    model.save(\"NN.h5\")\n",
        "\n",
        "Save_Model(model)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
